{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "01_TF2_MNIST_sequential.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxKTwo2u311n"
      },
      "source": [
        "이번 session 에세는 단순히 network 구조와 ensemble의 예제에 대해서만 이야기하도록 한다.\n",
        "\n",
        "기존의 keras model 에서는 network를 정의 하는 방식이 sequential 한 방법과 function에 의한 방법 두가지를 사용하였다. \n",
        "\n",
        "Tensorflow 2.0으로 오면서 subclass 를 사용하는 법이 추가 되어졌다.\n",
        "현재로서는 subclass 를 딱히 사용하여서 해결하여야 하는 경우를 찾아보기는 힘들다.\n",
        "Tensorflow 팀에서 어떠한 경우에 사용하도록 만들었는지는 알기 어렵지만, tensorflow 2.0의 큰 변화중 하나 이므로 예제를 추가한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw_BiL-Q5E4i"
      },
      "source": [
        "Sequential model은 두가지로 표현되어 질수 있다.\n",
        "이는 코딩하는 연구자가 편한것을 사용하면 된다.\n",
        "\n",
        "```\n",
        "\"\"\"\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME', \n",
        "                                  input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPool2D(padding='SAME'))\n",
        "        \n",
        "    model.add(Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n",
        "    model.add(MaxPool2D(padding='SAME'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation=tf.nn.relu))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10, activation=tf.nn.softmax))\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\"\"\"\n",
        "model = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdfROtFk6OF-",
        "outputId": "12fc1d06-6728-4fd0-99f5-bc712b2e6661"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
        "from tensorflow.keras import Model, Sequential\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "## MNIST Dataset #########################################################\n",
        "mnist = tf.keras.datasets.mnist\n",
        "class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "##########################################################################\n",
        "\n",
        "## Fashion MNIST Dataset #################################################\n",
        "# mnist = tf.keras.datasets.fashion_mnist\n",
        "# class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "##########################################################################\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()    \n",
        "\n",
        "# Change data type as float. If it is unt type, it might cause error \n",
        "X_train = X_train / 255.\n",
        "X_test  = X_test / 255.\n",
        "\n",
        "# in the case of Keras or TF2, type shall be [image_size, image_size, 1]\n",
        "# if it is RGB type, type shall be [image_size, image_size, 3]\n",
        "# For MNIST or Fashion MNIST, it need to reshape\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "    \n",
        "Y_train = to_categorical(Y_train, 10)\n",
        "Y_test = to_categorical(Y_test, 10)    \n",
        "    \n",
        "batch_size = 1000\n",
        "# 입력된 buffer_size만큼 data를 채우고 무작위로 sampling하여 새로운 data로 바꿉니다.\n",
        "# 완벽한 셔플링을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기가 필요합니다.\n",
        "# 만약 작은 데이터수보다 작은 buffer_size를 사용할경우,\n",
        "# 처음에 설정된 buffer_size만큼의 data안에서 임의의 셔플링이 발생합니다.\n",
        "shuffle_size = 100000\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_train, Y_train)).shuffle(shuffle_size).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_test, Y_test)).batch(batch_size)\n",
        "\n",
        "\"\"\"\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME', \n",
        "                                  input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPool2D(padding='SAME'))\n",
        "        \n",
        "    model.add(Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'))\n",
        "    model.add(MaxPool2D(padding='SAME'))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation=tf.nn.relu))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(10, activation=tf.nn.softmax))\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "\"\"\"\n",
        "model = Sequential([\n",
        "    Conv2D(filters=64, kernel_size=3, activation=tf.nn.relu, padding='SAME',input_shape=(28, 28, 1)),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Conv2D(filters=128, kernel_size=3, activation=tf.nn.relu, padding='SAME'),\n",
        "    MaxPool2D(padding='SAME'),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 878,730\n",
            "Trainable params: 878,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5ynVGDs6Y2k"
      },
      "source": [
        "아래의 code는 colab에서 제공하는 \"전문가를 위한 빠른 시작\" 부분을 약간 수정하였다.\n",
        "\n",
        "큰 의미는 부여하지 말고, 이번 notebook에서는 sequential model에 집중하여서 보자.\n",
        "sequential model은 간단한 모델에 적합하다.\n",
        "\n",
        "sequential model도 transfer learning 에 사용될수 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-YdHmWJCl6e",
        "outputId": "800f20b4-e9ee-46b2-82ef-7db5d7e5c5b9"
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def loss_fn(model, images, labels):\n",
        "    logits = model(images, training=True)\n",
        "    loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(\n",
        "        y_pred=logits, y_true=labels, from_logits=True))    \n",
        "    return loss   \n",
        "\n",
        "@tf.function\n",
        "def grad(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_fn(model, images, labels)\n",
        "    return tape.gradient(loss, model.variables)\n",
        "\n",
        "@tf.function\n",
        "def evaluate(model, images, labels):\n",
        "    logits = model(images, training=False)\n",
        "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(cnn=model)\n",
        "\n",
        "@tf.function\n",
        "def train(model, images, labels):\n",
        "    grads = grad(model, images, labels)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = 0.\n",
        "    test_loss = 0.\n",
        "    train_accuracy = 0.\n",
        "    test_accuracy = 0.\n",
        "    train_step = 0\n",
        "    test_step = 0    \n",
        "    \n",
        "    for images, labels in train_ds:\n",
        "        train(model, images, labels)\n",
        "        #grads = grad(model, images, labels)                \n",
        "        #optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        loss = loss_fn(model, images, labels)\n",
        "        train_loss += loss\n",
        "        acc = evaluate(model, images, labels)\n",
        "        train_accuracy += acc\n",
        "        train_step += 1\n",
        "    train_loss = train_loss / train_step\n",
        "    train_accuracy = train_accuracy / train_step\n",
        "    \n",
        "    for test_images, test_labels in test_ds:\n",
        "        loss = loss_fn(model, test_images, test_labels)\n",
        "        test_loss += loss\n",
        "        acc = evaluate(model, test_images, test_labels)        \n",
        "        test_accuracy += acc\n",
        "        test_step += 1\n",
        "    test_loss = test_loss / test_step\n",
        "    test_accuracy = test_accuracy / test_step\n",
        "    \n",
        "    \"\"\"\n",
        "    print('Epoch:', '{}'.format(epoch + 1), 'loss =', '{:.8f}'.format(train_loss), \n",
        "          'train accuracy = ', '{:.4f}'.format(train_accuracy), \n",
        "          'test accuracy = ', '{:.4f}'.format(test_accuracy))\n",
        "    \"\"\"\n",
        "    template = 'epoch: {:>5,d}, loss: {:>2.4f}, accuracy: {:>2.3f} %, test loss: {:>2.4f}, test accuracy: {:>2.3f} %'\n",
        "    print (template.format(epoch+1,\n",
        "                         train_loss,\n",
        "                         train_accuracy*100,\n",
        "                         test_loss,\n",
        "                         test_accuracy*100))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:     1, loss: 1.7104, accuracy: 80.657 %, test loss: 1.5474, test accuracy: 93.740 %\n",
            "epoch:     2, loss: 1.5263, accuracy: 95.330 %, test loss: 1.5102, test accuracy: 96.350 %\n",
            "epoch:     3, loss: 1.5037, accuracy: 97.057 %, test loss: 1.4931, test accuracy: 97.780 %\n",
            "epoch:     4, loss: 1.4917, accuracy: 97.938 %, test loss: 1.4880, test accuracy: 98.110 %\n",
            "epoch:     5, loss: 1.4872, accuracy: 98.263 %, test loss: 1.4863, test accuracy: 98.330 %\n",
            "Learning Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
